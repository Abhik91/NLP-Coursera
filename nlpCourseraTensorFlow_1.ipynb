{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpCourseraTensorFlow_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpskxQVf0GAnv4s5gn5BkP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhik91/NLP-Coursera/blob/master/nlpCourseraTensorFlow_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moyf-bwmsM1f",
        "colab_type": "text"
      },
      "source": [
        "# This is the first program using Tensor Flow from Coursera NLP Course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpW6ONjQsaD6",
        "colab_type": "text"
      },
      "source": [
        "**Description** - Using Tensorflow to tokenize words using Tokenizer library. Tokenizer does all the heavy lifting of managing tokens, turning text into streams of tokens etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7INjFFE5sUB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on1wK2DotahK",
        "colab_type": "text"
      },
      "source": [
        "2. Creating a list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoQT08BNs7DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = [ 'I love my dog',\n",
        "             'I love my cat'\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6kY_4Ft4NY",
        "colab_type": "text"
      },
      "source": [
        "3. Create an instance of ***Tokenizer***. Pass parameter ***num_words*** to it. Value provided is too large as we don't know which how many unique text are there in the sentence list. So by setting this hyper-parameter, Tokenizer will take top 100 words by volume and encode those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M4B3mYItlHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bEAgOYMue5X",
        "colab_type": "text"
      },
      "source": [
        "4. ***fit_on_texts*** method takes the words and tokenizes it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFTeAdjrtenz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLtTDTwlt1Hw",
        "colab_type": "text"
      },
      "source": [
        "5. The tokenizer provides a ***word_index*** property which returns a dictionary containing key value pairs. If you see the output, the I in the list sentence was capitalized, but in the output it is in small. Thus the tokenizer also ***strips the punctuation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cG7JrZQvD7d",
        "colab_type": "code",
        "outputId": "570e3dfa-cc3f-4c3e-c419-cd2fb8ec64f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print (word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_klg-Asvt52",
        "colab_type": "text"
      },
      "source": [
        "6. Lets take another example to verify how tokenizer ***strips the punctuation*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaQGzHxwvJO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = [\n",
        "            'I love my dog',\n",
        "            'I love my cat',\n",
        "            'You love my dog!'\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m519Ey7wFTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpesLFsSwWCJ",
        "colab_type": "text"
      },
      "source": [
        "7. On displaying the output, we can see that ***dog!*** is not treated as a different word, it is the same word and an addition of ***you***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRol9Q3IwKh_",
        "colab_type": "code",
        "outputId": "0f36aa9a-a724-49e2-8811-a93a3c7bdbaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print (word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_g_UZZW2P5M",
        "colab_type": "text"
      },
      "source": [
        "## Text to sequence\n",
        "Here we will be creating a list of sequences, the sentences encoded with the tokens that we generated "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RObaTwIr2qzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = [\n",
        "            'I love my dog',\n",
        "            'I love my cat',\n",
        "            'You love my dog!',\n",
        "            'She really loves my dog!!'\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veyvwQap29EG",
        "colab_type": "text"
      },
      "source": [
        "Notice the list above have a sentence which is of greater length than the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UupMC4D628Zj",
        "colab_type": "code",
        "outputId": "7604fa92-3672-427d-e8f8-7ff841d33dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.fit_on_texts(sentence)\n",
        "word_index = tokenizer.word_index\n",
        "print (word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6, 'she': 7, 'really': 8, 'loves': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZbUuqS_3SMU",
        "colab_type": "text"
      },
      "source": [
        "Now we will create list of sequences. Each sequence is the number correspoding to the word. Like the first one [4,2,1,3] is 'I love my dog' and so on..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlCaEjN3QWk",
        "colab_type": "code",
        "outputId": "ed9e9656-3fd7-450a-9130-356c111a129f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences= tokenizer.texts_to_sequences(sentence)\n",
        "print (sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 2, 1, 4], [3, 2, 1, 5], [6, 2, 1, 4], [7, 8, 9, 1, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diJPERGN422G",
        "colab_type": "text"
      },
      "source": [
        "***Disadvantage is if you give a list of words on pre-trained model and the words are not present, it will create sequence with those tokens available to it, thus creating incorrect sequence***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Vv170Y3eS2",
        "colab_type": "code",
        "outputId": "f6484f58-89d4-4fe9-cdb5-7f1e32418007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_sentences = [\n",
        "            'I love my dog',\n",
        "            'She really does not love my dog!!'\n",
        "            ]\n",
        "sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "print (sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 2, 1, 4], [7, 8, 2, 1, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdeANd0v41i9",
        "colab_type": "text"
      },
      "source": [
        "***To overcome the disadvantage we will use the oov feature of tokenizer. OOV or out of vocabulary, replaces unseen words or vocabulary with oov string. OOV string should be unique and should'nt be among the vocabulary like '\\<OOV\\>'***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkpH4YXoU_AU",
        "colab_type": "code",
        "outputId": "9c0320d4-ef07-406e-f27b-4a4e28ba04a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tokenizer1 = Tokenizer(num_words=100, oov_token='<OOV>')\n",
        "tokenizer1.fit_on_texts(sentence) #Training tokenizer with the sentences in the list sentence\n",
        "word_index = tokenizer1.word_index\n",
        "print (word_index)\n",
        "\n",
        "sequences = tokenizer1.texts_to_sequences(test_sentences)#Testing tokenizer with the sentences in the list sentence\n",
        "print (sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'cat': 6, 'you': 7, 'she': 8, 'really': 9, 'loves': 10}\n",
            "[[5, 3, 2, 4], [8, 9, 1, 1, 3, 2, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Qv64vGY7s6",
        "colab_type": "text"
      },
      "source": [
        "***Padding***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln-5KQ3fYoD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK-aIraJZEa-",
        "colab_type": "code",
        "outputId": "1c4bc6cf-5e2f-46b8-d949-8af3855eca4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "padded = pad_sequences(sequences)\n",
        "print (padded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 5 3 2 4]\n",
            " [8 9 1 1 3 2 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT8LhLV8ZxL9",
        "colab_type": "text"
      },
      "source": [
        "***Padding at the end of sentences***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkTJja7tZJfT",
        "colab_type": "code",
        "outputId": "70a0a3ae-f079-48e0-adb9-cb3f72be8983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "padded1 = pad_sequences(sequences, padding = 'post')\n",
        "print (padded1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 3 2 4 0 0 0]\n",
            " [8 9 1 1 3 2 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr9p9egEZ2d5",
        "colab_type": "text"
      },
      "source": [
        "***Padding with maxlen = 5*** -> Removes the words from the begining of the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_JEu83pZtFT",
        "colab_type": "code",
        "outputId": "fde6ab8b-6d85-4852-bbb0-a063b96e8154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "padded2 = pad_sequences(sequences, padding = 'post', maxlen=5)\n",
        "print (padded2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 3 2 4 0]\n",
            " [1 1 3 2 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKSyz15PaSZ0",
        "colab_type": "text"
      },
      "source": [
        "***Padding with maxlen = 5*** -> Remove the words from the end of the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9C-NWhxZ9oL",
        "colab_type": "code",
        "outputId": "3f84288f-15ed-4020-b120-3bd32a195802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "padded3 = pad_sequences(sequences, padding = 'post', maxlen=5, truncating='post')\n",
        "print (padded3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 3 2 4 0]\n",
            " [8 9 1 1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}